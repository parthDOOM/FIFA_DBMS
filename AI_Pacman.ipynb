{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUDB3Ow5r16+uMue4q9lwK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parthDOOM/FIFA_DBMS/blob/master/AI_Pacman.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca4OXHPBrgC3",
        "outputId": "33a1e1c3-09ef-4d9f-e17e-23bd35ce3078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: autorom~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: shimmy<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari]) (0.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (4.66.5)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (0.6.1)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari]) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari]) (6.4.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2024.8.30)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "swig is already the newest version (4.0.2-1ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.0)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.1)\n"
          ]
        }
      ],
      "source": [
        "# Installing the required packages and importing the libraries\n",
        "\n",
        "### Installing Gymnasium and required packages\n",
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!apt-get install -y swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Importing the libraries\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "M0RGurE2rzsX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1 - Building the AI\n",
        "\n",
        "### Creating the architecture of the Neural Network\n",
        "# This network will process the input state from the environment\n",
        "# and output Q-values for each action.\n",
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self, action_size, seed=42):\n",
        "        super(Network, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.fc1 = nn.Linear(10 * 10 * 128, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.bn1(self.conv1(state)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "I0BdaDN9r29J"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2 - Training the AI\n",
        "\n",
        "### Setting up the environment\n",
        "import gymnasium as gym\n",
        "env = gym.make('MsPacmanDeterministic-v0', full_action_space=False)\n",
        "state_shape = env.observation_space.shape\n",
        "state_size = env.observation_space.shape[0]\n",
        "number_actions = env.action_space.n\n",
        "print('State shape: ', state_shape)\n",
        "print('State size: ', state_size)\n",
        "print('Number of actions: ', number_actions)\n",
        "\n",
        "### Initializing the hyperparameters\n",
        "learning_rate = 1e-4\n",
        "minibatch_size = 64\n",
        "discount_factor = 0.99"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyEJypilr6VX",
        "outputId": "f2296db2-c7a9-4284-a377-74942aeaae79"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State shape:  (210, 160, 3)\n",
            "State size:  210\n",
            "Number of actions:  9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing the frames before passing to the network\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "def preprocess_frame(frame):\n",
        "    frame = Image.fromarray(frame)\n",
        "    preprocess = transforms.Compose([transforms.Resize((128, 128)), transforms.ToTensor()])\n",
        "    return preprocess(frame).unsqueeze(0)"
      ],
      "metadata": {
        "id": "PqGAySThr9MZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Implementing the DCQN class\n",
        "# The Agent will interact with the environment and learn using the Q-network.\n",
        "\n",
        "class Agent():\n",
        "\n",
        "    def __init__(self, action_size):\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.action_size = action_size\n",
        "        self.local_qnetwork = Network(action_size).to(self.device)\n",
        "        self.target_qnetwork = Network(action_size).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr=learning_rate)\n",
        "        self.memory = deque(maxlen=10000)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        state = preprocess_frame(state)\n",
        "        next_state = preprocess_frame(next_state)\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        if len(self.memory) > minibatch_size:\n",
        "            experiences = random.sample(self.memory, k=minibatch_size)\n",
        "            self.learn(experiences, discount_factor)\n",
        "\n",
        "    def act(self, state, epsilon=0.):\n",
        "        state = preprocess_frame(state).to(self.device)\n",
        "        self.local_qnetwork.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.local_qnetwork(state)\n",
        "        self.local_qnetwork.train()\n",
        "        if random.random() > epsilon:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, discount_factor):\n",
        "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "        states = torch.from_numpy(np.vstack(states)).float().to(self.device)\n",
        "        actions = torch.from_numpy(np.vstack(actions)).long().to(self.device)\n",
        "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(self.device)\n",
        "        next_states = torch.from_numpy(np.vstack(next_states)).float().to(self.device)\n",
        "        dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(self.device)\n",
        "        next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        q_targets = rewards + discount_factor * next_q_targets * (1 - dones)\n",
        "        q_expected = self.local_qnetwork(states).gather(1, actions)\n",
        "        loss = F.mse_loss(q_expected, q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ],
      "metadata": {
        "id": "0pNr38rvr_9f"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Initializing the DCQN agent\n",
        "agent = Agent(number_actions)"
      ],
      "metadata": {
        "id": "zw3PM4gvsDqH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Training the DCQN agent\n",
        "number_episodes = 50  # Reduced for quick testing\n",
        "maximum_number_timesteps_per_episode = 1000\n",
        "epsilon_starting_value  = 1.0\n",
        "epsilon_ending_value  = 0.01\n",
        "epsilon_decay_value  = 0.995\n",
        "epsilon = epsilon_starting_value\n",
        "scores_on_100_episodes = deque(maxlen=100000)\n",
        "\n",
        "for episode in range(1, number_episodes + 1):\n",
        "    state, _ = env.reset()\n",
        "    score = 0\n",
        "    for t in range(maximum_number_timesteps_per_episode):\n",
        "        action = agent.act(state, epsilon)\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        agent.step(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        score += reward\n",
        "        if done:\n",
        "            break\n",
        "    scores_on_100_episodes.append(score)\n",
        "    epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end=\"\")\n",
        "    if episode % 100 == 0:\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n",
        "    if np.mean(scores_on_100_episodes) >= 500.0:\n",
        "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores_on_100_episodes)))\n",
        "        torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc4ku_nIsFR_",
        "outputId": "544909ff-79e1-4fc5-eb96-02e47142b546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 9\tAverage Score: 237.78"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3 - Visualizing the results\n",
        "\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "def show_video_of_model(agent, env_name):\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        action = agent.act(state)\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "    env.close()\n",
        "    imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, 'MsPacmanDeterministic-v0')\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ],
      "metadata": {
        "id": "uY6Rbx1XsMJO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}